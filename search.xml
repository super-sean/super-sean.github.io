<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spring cloud stream 基于kafka的使用简析]]></title>
    <url>%2F2018%2F08%2F11%2Fspring-cloud-stream-%E5%9F%BA%E4%BA%8Ekafka%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[流式数据故名思义，即数据像开了小河里的流水般不停流动，除非水源出现问题，否则没有结束时间。流式数据在行业内已经有非常多针对不同应用量级的成熟方案，这里就不加以详述。本次主要介绍spring cloud stream 基于kafka对流式数据的基本应用。而使用spring cloud stream之前，可以先理解一下spring cloud对数据流程的几个概念，分别是source（生产数据者），processor(数据加工者), sink(最终结果处理者)。 准备工作项目基本框架:当然是基于maven构建的spring-boot最省心省力啦添加依赖:1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka-streams&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 用于代码中的一些便捷注解 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 公共配置:1234567891011121314151617181920spring: cloud: kafka: streams: binder: brokers: localhost:9092 zk-nodes: localhost:2181 #2.0以上就不需要该配置 configuration: default: key: serde: org.apache.kafka.common.serialization.Serdes$StringSerde value: serde: org.apache.kafka.common.serialization.Serdes$StringSerde cache: max: bytes: buffering: 0 #所有线程可以用于缓存的最大字节数,达到多少数据量之后聚合一次流中的数据，设置为0则实时聚合 commit: interval: ms: 1000 #版本数据确认消费时间ack 我们来模拟用户页面访问记录流 声明binding相关信息添加接口AnalyticsBinding1234567891011121314151617181920public interface AnalyticsBinding &#123; String PAGE_VIEWS_OUT = &quot;pvout&quot;; String PAGE_VIEWS_IN = &quot;pvin&quot;; String PAGE_COUNT_MV = &quot;pcmv&quot;; String PAGE_COUNT_OUT = &quot;pcout&quot;; String PAGE_COUNT_IN = &quot;pcin&quot;; @Input(PAGE_VIEWS_IN) KStream&lt;String,PageViewEvent&gt; pageViewsIn(); @Output(PAGE_COUNT_OUT) KStream&lt;String,Long&gt; pageCountOut(); @Input(PAGE_COUNT_IN) KTable&lt;String,Long&gt; pageCountIn(); @Output(PAGE_VIEWS_OUT) MessageChannel pageViewsOut();&#125; 添加配置在application.yml中声明以下配置12345678910111213141516171819202122232425262728293031323334353637spring: cloud: stream: bindings: pvout: destination: pvst group: pvst producer: header-mode: raw pvin: destination: pvst group: pvst consumer: header-mode: raw pcout: destination: pcst group: pcst producer: use-native-encoding: true pcin: destination: pcst group: pcst content-type: application/json consumer: use-native-encoding: true header-mode: raw kafka: streams: bindings: pcout: producer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde pcin: consumer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde destination对应kafka中的主题。serde就是serialization和deserialization，针对流中的key和value都需要指定，默认使用default中配置的内容，也可以针对主题单独设置。 source创建每秒随机产生用户访问页面及停留时间的数据 先创建事件类1234567@Data@AllArgsConstructor@NoArgsConstructorpublic class PageViewEvent &#123; private String userId,page; private long duration;&#125; 生成数据的实现123456789101112131415161718192021222324252627282930313233@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class PageViewEventSource implements ApplicationRunner &#123; private final MessageChannel pageViewsOut; public PageViewEventSource(AnalyticsBinding binding) &#123; this.pageViewsOut = binding.pageViewsOut(); &#125; @Override public void run(ApplicationArguments applicationArguments) throws Exception &#123; List&lt;String&gt; names = Arrays.asList(&quot;jlong&quot;,&quot;dyser&quot;,&quot;shacko&quot;,&quot;abilan&quot;,&quot;ooasdf&quot;,&quot;grussell&quot;); List&lt;String&gt; pages = Arrays.asList(&quot;blog&quot;,&quot;sitemap&quot;,&quot;initializr&quot;,&quot;news&quot;,&quot;colophon&quot;,&quot;about&quot;); Runnable runnable = () -&gt; &#123; String rPage = pages.get(new Random().nextInt(pages.size())); String rName = names.get(new Random().nextInt(names.size())); PageViewEvent pageViewEvent = new PageViewEvent(rName,rPage,Math.random() &gt; .5?10:1000); Message&lt;PageViewEvent&gt; message = MessageBuilder.withPayload(pageViewEvent) .setHeader(KafkaHeaders.MESSAGE_KEY,pageViewEvent.getUserId().getBytes()) .build(); try &#123; this.pageViewsOut.send(message); log.info(&quot;sent&quot; + message.toString()); &#125; catch (Exception e)&#123; log.error(e.getMessage()); &#125; &#125;; Executors.newScheduledThreadPool(1).scheduleAtFixedRate(runnable,1,1, TimeUnit.SECONDS); &#125;&#125; processor获取到事件数据之后基于聚合处理，创建新的数据流123456789101112131415@Slf4j@Componentpublic class PageViewEventProcessor &#123; @StreamListener @SendTo(AnalyticsBinding.PAGE_COUNT_OUT) public KStream&lt;String,Long&gt; process( @Input(AnalyticsBinding.PAGE_VIEWS_IN) KStream&lt;String,PageViewEvent&gt; events)&#123; return events .map((key,value) -&gt; new KeyValue&lt;&gt;(value.getUserId() + &quot;-&quot; + value.getPage(),&quot;0&quot;)) .groupByKey() //.windowedBy(TimeWindows.of(1000*60)) .count(Materialized.as(AnalyticsBinding.PAGE_COUNT_MV)) .toStream(); &#125;&#125; windowedBy可以根据时间窗口进行聚合，用法请详见文档。 sink获取聚合后的结果进行处理123456789@Slf4j@Componentpublic class PageCountSink &#123; @StreamListener public void process(@Input(AnalyticsBinding.PAGE_COUNT_IN)KTable&lt;String,Long&gt; counts)&#123; counts.toStream().foreach((key,value) -&gt; log.info(&quot;PCIN -----:&quot; + key + &quot;=&quot; + value)); &#125;&#125; 至此一个从数据生产到结果消费的简单数据流处理就完成了 解析这个例子是基于spring cloud 完整的流数据处理，有source,processor,sink的概念是spring cloud data flow的设计理念，这里不展开阐述。processor环节非必需的，可以只有source和sink的实现。假如不需要进行流的处理，只需要消息内容，可以在@StreamListener的方法声明中不使用@Input声明，而是直接通过@StreamListener(主题名称)来进行监听，方法接收消息参数使用Message msg，如下: 123456789@StreamListener(Processor.INPUT) public void receive1(Message&lt;String&gt; msg)&#123; System.out.println(msg.getPayload()); //消息体 Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class); if (acknowledgment != null) &#123; System.out.println(&quot;Acknowledgment provided&quot;); acknowledgment.acknowledge();//手动ack &#125; &#125; 多流聚合在实际应用中不只存在单流数据的处理，也经常会遇到多流聚合处理。我们来添加多一种事件的实现，这里模拟销售数据。相关的实现如下 SalesEvent1234567@Data@AllArgsConstructor@NoArgsConstructorpublic class SalesEvent &#123; private String userId,goods; private int amount;&#125; SalesEventSource12345678910111213141516171819202122232425262728293031@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class SalesEventSource implements ApplicationRunner &#123; private final MessageChannel salesOut; public SalesEventSource(AnalyticsBinding binding) &#123; this.salesOut = binding.salesOut(); &#125; @Override public void run(ApplicationArguments applicationArguments) throws Exception &#123; List&lt;String&gt; names = Arrays.asList(&quot;jlong&quot;,&quot;dyser&quot;,&quot;shacko&quot;,&quot;abilan&quot;,&quot;ooasdf&quot;,&quot;grussell&quot;); List&lt;String&gt; goods = Arrays.asList(&quot;apple&quot;,&quot;oringe&quot;,&quot;banana&quot;,&quot;lemon&quot;,&quot;shit&quot;,&quot;book&quot;); Runnable runnable = () -&gt; &#123; String rGoods = goods.get(new Random().nextInt(goods.size())); String rName = names.get(new Random().nextInt(names.size())); SalesEvent salesEvent = new SalesEvent(rName,rGoods,Math.random() &gt; .5?5:10); Message&lt;SalesEvent&gt; message = MessageBuilder.withPayload(salesEvent) .setHeader(KafkaHeaders.MESSAGE_KEY,salesEvent.getUserId().getBytes()) .build(); try &#123; this.salesOut.send(message); log.info(&quot;sent&quot; + message.toString()); &#125; catch (Exception e)&#123; log.error(e.getMessage()); &#125; &#125;; Executors.newScheduledThreadPool(1).scheduleAtFixedRate(runnable,1,1, TimeUnit.SECONDS); &#125;&#125; SalesEventProcessor123456789101112131415@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class SalesEventProcessor &#123; @StreamListener @SendTo(AnalyticsBinding.SALES_COUNT_OUT) public KStream&lt;String,Long&gt; process(@Input(AnalyticsBinding.SALES_IN)KStream&lt;String,SalesEvent&gt; events)&#123; return events .map((key,value) -&gt; new KeyValue&lt;&gt;(value.getUserId() + &quot;-&quot; + value.getGoods(),&quot;0&quot;)) .groupByKey() //.windowedBy(TimeWindows.of(1000*60)) .count(Materialized.as(AnalyticsBinding.SALES_COUNT_MV)) .toStream(); &#125;&#125; SaleCountSink123456789101112131415161718@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class SaleCountSink &#123; @StreamListener public void process(@Input(AnalyticsBinding.SALES_COUNT_IN)KTable&lt;String,Long&gt; salesCounts, @Input(AnalyticsBinding.PAGE_COUNT_IN)KTable&lt;String,Long&gt; pageCounts)&#123; salesCounts.toStream().map((k,v) -&gt; new KeyValue&lt;&gt;(k.split(&quot;-&quot;)[0],k.split(&quot;-&quot;)[1] + &quot;-&quot; + v)) .join(pageCounts.toStream().map((k,v) -&gt; &#123; System.out.println(&quot;-------&quot; + k +&quot; : &quot; + v ); return new KeyValue&lt;&gt;(k.split(&quot;-&quot;)[0],k.split(&quot;-&quot;)[1] + &quot;-&quot; + v);&#125;), (v1, v2) -&gt; v1 + &quot;:&quot; + v2, JoinWindows.of(10000) ) .foreach((k,v) -&gt; System.out.println(k+ &quot;---&quot; + v)); &#125;&#125; 注意：同个应用中对同个流的监听实例只能有一个，SaleCountSink使用了@Input(AnalyticsBinding.PAGE_COUNT_IN)KTable&lt;String,Long&gt;与PageCountSink有冲突，要嘛把PageCountSink中的监听去掉，要嘛重命名其中一个相关的监听配置 AnalyticsBinding 完整代码12345678910111213141516171819202122232425262728293031323334353637public interface AnalyticsBinding &#123; String PAGE_VIEWS_OUT = &quot;pvout&quot;; String PAGE_VIEWS_IN = &quot;pvin&quot;; String PAGE_COUNT_MV = &quot;pcmv&quot;; String PAGE_COUNT_OUT = &quot;pcout&quot;; String PAGE_COUNT_IN = &quot;pcin&quot;; String SALES_OUT = &quot;salesout&quot;; String SALES_IN = &quot;salesin&quot;; String SALES_COUNT_MV = &quot;scmv&quot;; String SALES_COUNT_OUT = &quot;scout&quot;; String SALES_COUNT_IN = &quot;scin&quot;; @Input(PAGE_VIEWS_IN) KStream&lt;String,PageViewEvent&gt; pageViewsIn(); @Output(PAGE_COUNT_OUT) KStream&lt;String,Long&gt; pageCountOut(); @Input(PAGE_COUNT_IN) KTable&lt;String,Long&gt; pageCountIn(); @Output(PAGE_VIEWS_OUT) MessageChannel pageViewsOut(); @Output(SALES_OUT) MessageChannel salesOut(); @Input(SALES_IN) KStream&lt;String,SalesEvent&gt; salesIn(); @Output(SALES_COUNT_OUT) KStream&lt;String,Long&gt; salesCountOut(); @Input(SALES_COUNT_IN) KTable&lt;String,Long&gt; salesCountIn();&#125; application.yml完整配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990server: port: 8388spring: application: name: input-demo cloud: instance-count: 1 instance-index: 0 stream: bindings: pvout: destination: pvst group: pvst producer: header-mode: raw pvin: destination: pvst group: pvst consumer: header-mode: raw pcout: destination: pcst group: pcst producer: use-native-encoding: true pcin: destination: pcst group: pcst content-type: application/json consumer: use-native-encoding: true header-mode: raw salesout: destination: sost group: sost producer: header-mode: raw salesin: destination: sost group: sost consumer: header-mode: raw scout: destination: scst group: scst producer: use-native-encoding: true scin: destination: scst group: scst content-type: application/json consumer: use-native-encoding: true header-mode: raw kafka: streams: binder: configuration: default: key: serde: org.apache.kafka.common.serialization.Serdes$StringSerde value: serde: org.apache.kafka.common.serialization.Serdes$StringSerde cache: max: bytes: buffering: 0 commit: interval: ms: 1000 brokers: localhost:9092 zk-nodes: localhost:2181 bindings: pcout: producer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde pcin: consumer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde scout: producer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde scin: consumer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde 多流合并的理念是把流两两之间的key处理成一样进行join处理，join的实现方法大家可以自行阅读文档。如果是三个流以上，需要先将两个流合并之后生成一个流再与第三流合并处理。 相关文档 spring cloud data flow spring cloud stream spring cloud stream kafka streams应用官方视频讲解 KTable与KStream的关系 kafka官方文档 kafka streams window的概念翻译版 kafka权威指南]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>实时流</tag>
        <tag>spring</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存与JVM]]></title>
    <url>%2F2018%2F07%2F23%2F%E5%86%85%E5%AD%98%E4%B8%8EJVM%2F</url>
    <content type="text"><![CDATA[自己一边看书一边实践一边整理下来的知识思维导向图，个人感觉能够掌握自动内存管理机制、高效并发并加以应用，理解虚拟机执行子系统、程序编译与代码优化，才可以说熟悉JVM。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>内存</tag>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zk分布式任务队列交互设计]]></title>
    <url>%2F2018%2F07%2F22%2Fzk%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97%E4%BA%A4%E4%BA%92%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[项目地址：https://github.com/super-sean/task-pipeline 背景近来公司业务上越来越多的跨进程比较耗时计算的场景出现，想用异步通信来解决长时间资源占用及等待问题，而基于多方的探讨，不考虑采用mina和netty这种异步通信的框架，最后决定使用zookeeper来实现 目的异步进行耗时较长的复杂计算请求，可随时获取请求执行进度 实现思路 将这种请求的发起者当作是请求任务的生产者，每个请求其实就是一个计算任务。 后端接收请求的服务就是消费者，获得请求之后进行计算，并更新计算进度。 当任务完成时，请求发起者可以通过监听任务状态回调实现自己的逻辑。 过程中，请求发起者也可以主动获取计算讲求的进度。 实现设计基于实现思路，设计zk的path结构如下 /master为程序高可用实现预留路径 /apps为业务连接节点，底下结构为/app/node，比如你有个业务叫a,有两个业务节点b1和b2，那就有/a/b1和/a/b2 路径。由业务节点启动时注册 /workers底下结构逻辑与/apps一致，只不过节点为服务端的节点，由服务端节点启动时注册 /tasks由业务提交注册的计算任务,以业务区分目录，以app-node-timestamp格式来命名taskid,每个节点拥有params,status和result三个节点 params 为请求参数，以文本格式存储，例如可以使用json格式传输 status 为task状态，默认有submit,running,done,noworker（无计算服务）,missapp（app节点断线）,consumed（已消费），resubmit（重分配）几种状态，worker可以添加自定义中间过程状态，任务提交时默认为submit状态。 result 为初始不存在，当status变更为done时添加，内容为文本格式，例如可以使用json，包括type和value,先只支持两种，第一种为直接返回为{“type”:”content”,”value”:”something”},考虑zk单个节点的容量问题，可能返回较大数据量，使用redis作为结果缓存层，返回{“type”:”redis_key”,”value”:”one redis key”} 当然不用redis也行，当数据量更大的时候可使用其它工具，这里先选用redis history目录下为完成的任务，定时持久化清理。 /assign由系统根据业务app分配作业给worker，以node-taskid来标识作业history目录下为执行完的作业，定时持久化清理 模块设计 调度系统 实现基于zk的路径交互，负责与业务和服务两端交互 业务端接口包封装 对于业务端来说，只需要提交服务端接口标识，接口参数之后返回taskId,根据需要通过taskId进行结果回调监听，支持查询task状态，需要屏蔽底层操作，透明化复杂操作。 服务端接口包封装 对于服务端来说，只需要继承某个类，声明服务标识，实现监听task队列的方法，处理被推送过来的任务，并根据需要更新自定义task状态，处理完成后在方法选择返回的内容类型即可 流程设计正常交互流程(由于用的uml画图工具问题，画得不是很规范，见谅…)正常交互流程worker断线重新分配任务流程 核心模块类图基本操作都抽象成名为operation的类，基于不同角色做扩展，目前情况如下baseOperation为zk的基本操作，operation为倾向原子性业务操作，分角色扩展的operation如serverOperation为封装角色实现本身的组合操作监听器主要有以下监听器实现每个角色都是基于以上两个核心模块加以逻辑处理来实现自己的功能 其它相关设计Task分发策略worker每当被分发task，便权重添加1，处理完则减1分发Task时选择权重最小的节点若权重都一样，则选择第一个节点 server主从实现使用curator包的LeaderLatch zk path acl权限管理使用三个角色，tp_server,tp_worker,tp_app目前没有做细粒度控制，只是tp_server创建的给另外两个角色授权，tp_worker创建的给tp_server授权，tp_app创建的给tp_server授权]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>分布式</tag>
        <tag>队列</tag>
      </tags>
  </entry>
</search>
