<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JAVA动态编译/解析文本的一种简易方法]]></title>
    <url>%2F2018%2F10%2F07%2FJAVA%E5%8A%A8%E6%80%81%E7%BC%96%E8%AF%91-%E8%A7%A3%E6%9E%90%E6%96%87%E6%9C%AC%E7%9A%84%E7%AE%80%E6%98%93%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[&ensp; &ensp; 追着国庆假期的尾巴，更新一下博客，讲一下项目上之前遇到的文本动态编译/解析的问题，虽然比较简单，但是感觉适合场景还是挺多的。&ensp; &ensp; 团队自研BI系统，在数据源选择及查询的实现使用桥接模式，针对不同的数据源采用不同的查询方式，而目前我们平台支持的数据源的查询可以通过构建不同的文本请求体进行查询，例如mysql、presto、ElasticSearch等。在这里对文本动态构建的实现方案进行讲述。&ensp; &ensp; 我们的源文本都是基于xml标签编写，起因是BI系统一开始是基于mybatis对mysql数据库进行DAO操作。举个栗子：12345678910&lt;select id=&quot;test&quot; parameterType=&quot;map&quot; resultType=&quot;java.util.HashMap&quot;&gt;select id from table_test where 1=1 &lt;if test=&quot;startTime != null and startTime != &apos;&apos;&quot;&gt; and sale_date &gt;= #&#123;startTime&#125; &lt;/if&gt; &lt;if test=&quot;endTime != null and endTime != &apos;&apos;&quot;&gt; and #&#123;endTime&#125; &gt;= sale_date &lt;/if&gt;&lt;/select&gt; &ensp; &ensp; 在这个基础上我们了解了mybatis的解析过程，使用mybatis的底层实现进行动态sql语句文本编译,其实原理就是利用mybatis的xml标签解析sql实现。&ensp; &ensp; xml动态文本编译/解析有多种方法，但是基于以上的情况，我们抽取了mybatis的boundsql编译过程用来构建我们的文本请求体12345678910111213 //使用mybatis编译逻辑标签 Document doc = DOMUtils.parseXMLDocument(query); XPathParser xPathParser = new XPathParser(doc, false); Node node = doc.getFirstChild(); XNode xNode = new XNode(xPathParser, node, null); XMLScriptBuilder xmlScriptBuilder = new XMLScriptBuilder(configuration, xNode); SqlSource sqlSource = xmlScriptBuilder.parseScriptNode(); BoundSql boundSql = sqlSource.getBoundSql(param);&ensp; &ensp; 通过boundSql.getSql()可以获取编译后的文本（configuration是mybatis的基础配置类，由于这里并不用到数据库请求，所以创建一个新的单例对象传入就可以），但是需要注意的是mybatis这种编译方式是用来编译prepare statment的，什么意思呢，就是使用mybatis的标签变量声明规则，即变量是#{var}这种声明方式，会被编译成问号占位符，如果不想这样可以自己制定变量规则进行替换，比如${var}123 boundSql.getSql().indexOf(&quot;$&quot;) &gt; 0 ? replaceVariables(boundSql.getSql(),param) : boundSql.getSql(); &ensp; &ensp; 到此我们的presto动态语句可以写成1234567891011 &lt;select id=&quot;test&quot; parameterType=&quot;map&quot; resultType=&quot;java.util.HashMap&quot;&gt;select bill_id,total from platform_data.t_sales_bill where 1=1 &lt;if test=&quot;startTime != null and startTime != &apos;&apos;&quot;&gt; and sale_date &gt;= cast(#&#123;startTime&#125; as timestamp) &lt;/if&gt; &lt;if test=&quot;endTime != null and endTime != &apos;&apos;&quot;&gt; and cast(#&#123;endTime&#125; as timestamp) &gt;= sale_date &lt;/if&gt;&lt;/select&gt; &ensp; &ensp; es的请求体可以写成1234567891011121314151617181920212223242526272829 &lt;query&gt;&#123; &quot;query&quot; : &#123; &quot;bool&quot;:&#123; &quot;filter&quot;:[ &lt;if test=&quot;agentCodes!=null&quot;&gt; &#123;&quot;match&quot; : &#123; &quot;agentCode&quot; : &quot;$&#123;agentCodes&#125;&quot;&#125; &#125;, &lt;/if&gt; &#123;&quot;range&quot; : &#123; &quot;saleDate&quot; : &#123;&quot;gte&quot;:&quot;$&#123;startTime&#125;&quot;,&quot;lte&quot;:&quot;$&#123;endTime&#125;&quot;&#125; &#125; &#125; ] &#125; &#125;, &quot;aggs&quot;:&#123; &quot;sales_number&quot;:&#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;goodsId&quot;, &quot;order&quot;:&#123; &quot;salesNumber&quot;:&quot;desc&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;salesNumber&quot;:&#123;&quot;sum&quot;:&#123; &quot;field&quot;:&quot;salesNumber&quot; &#125; &#125;, &quot;salesTotal&quot;:&#123;&quot;sum&quot;:&#123; &quot;field&quot;:&quot;salesTotal&quot; &#125; &#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123; &quot;salesNumber&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125; &#125;&lt;/query&gt;&ensp; &ensp; 同理其它可以使用文本构建查询的数据源都可以支持，比如Hql,spark-sql,ksql等，再扩展也可以支持动态脚本。为我们自助查询平台和BI平台奠定了基础。&ensp; &ensp; 以上，说得比较简单，希望对看到的人可以有帮助。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java基础</tag>
        <tag>mybatis</tag>
        <tag>动态解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人基于大数据平台的设计与思考]]></title>
    <url>%2F2018%2F08%2F19%2F%E4%B8%AA%E4%BA%BA%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[背景介绍&ensp; &ensp; 本人是在一家TOB电商平台的创业公司就职，目前负责整个数据部门的架构与业务跟进。之前从事过YY与唯品会的web开发工作，也有了解到一些大数据架构相关的设计。&ensp; &ensp; 目前数据部门的技术架构发展大体方向已经确定得差不多，于是在此梳理一下设计思路、历程与将要实现的内容。 一开始都是坑&ensp; &ensp; 一开始的数据需求都是来源于生意运营的报表，我由负责电商app web转向数据开发，说实话，一开始做报表我心里是拒绝的，但是创业团队为了公司能活下去，什么活不得做。领着几个兄弟搭建了公司的”数据中心”，其实就是个报表系统。当时对hadoop,hive,spark,etl等技术及概念懵懵懂懂。 &ensp; &ensp; 很快我们发现，第一轮的坑来了，面临两个问题：一，报表需求多，小伙伴们整天在写sql，代码组合数据；二，数据量慢慢大起来，有些地方性能比较差，报表要查很久，有时候影响线上性能，线上出问题总要挨白眼。 &ensp; &ensp; 经过了解对比，我们毅然决定开始自研BI系统(参照superset功能设计)和开启我们的ETL架构设计与实现的开头，独立一套数据环境。其实当时我们心中还有点窃喜，终于可以接触高大上的hadoop全家桶了，选取的技术有hadoop、hive、spark、sqoop和shell脚本来实现我们的第一版数据流程。嗯，这才有点数据中心的样子。 &ensp; &ensp; 然而，跑了一段时间后，随着了解越多知识，才知道我们的情况有多糟糕，当时数据量不到100g，只有两台8核16G的机器（手动捂脸），离线任务依靠时间弱依赖，就算只有两台，运维起来也要花不少时间，sqoop在小数据量小集群的应用场景下性能真的很一般啊，渐渐感觉到这套架构对我们来说也许”太早了”。自研BI方面，很考验写sql的功力，虽然由此我们对mysql的sql编写技巧有很大提升，用了很多奇淫巧技，但是，我们的研发现在都花很多时间写sql啊，而且大家对数据不敏感，反正就是完成开发任务就得了。这段时间确实人心很不稳。。。当然很感谢那时坚持陪着我奋斗的小伙伴们。 初版架构如下: 有坑就要填思考、计划、实现与招聘，我们在1个月内重塑了整个交互流程。 数据通过爬虫、收集日志与第三方公司进行业务结合等方式慢慢积累起来 招来第一位数据分析师，并由他负责BI的输出与日常报表需求 申请多了几台机器资源 用azkaban替换shell脚本管理，正式接上任务流 使用datax替换sqoop hadoop、hive及spark主要用来做少量智能化的业务，大部分数据聚合逻辑回迁数据库 这样的数据流程立马解放了开发小伙伴的双手，去做更能体现价值的事情，而且报表输出的质量也大大提高了，机器资源也暂时足够，维护起来还不算特别麻烦。心里舒了一口气。 架构如下: 业务不断发展，坑坑更健康&ensp; &ensp; 这个时候我们的业务数据量已经超过100-200G之间，数据中心的数据量也300g左右，数据库中的中间表特别多。&ensp; &ensp; 现在已经好几位分析师，各自负责自己的需求，经常有数据打架的情况发生，而且制作报表很多依然用的业务源表，有性能问题就使用中间表。&ensp; &ensp; 业务应用越来越多对数据聚合服务有需求，我们都是通过RPC接口来对外服务提供服务,我们经常得深入业务逻辑去提供聚合服务，而且服务对象多，所有服务在数据库聚合，经常影响数据中心整体的服务质量，也出现过多次应用雪崩的情况。&ensp; &ensp; 业务也出现了实时反馈的产品需求 不断学习探研改造我们又再一次进行架构改造 接入otter实现业务数据实时同步 使用canal + kafka + kafka streams 实现实时数据变更订阅，提供实时聚合服务 开始设计自己的数据字典与数据仓库 采用微服务设计理念，使用spring cloud 架构如下: 就着这样节奏又走了一段日子 总是会有不如意的地方&ensp; &ensp; 此时我们的机器资源也只有近10台一般配置的节点，随着微服务越来越多，而且与其它大数据的工具平台共用，很快又到瓶颈，而且进程多维护起来也是麻烦。&ensp; &ensp; 数据团队的人员由于资源限制没有再新增，而业务缺越来越多需要支持，在生产力方面，数据处理环节明显成为瓶颈。&ensp; &ensp; 此时在数据聚合方面，我们采用了redis作为缓存，但是依然存在缓存击破的情况，导致数据库偶尔会锁表，即使我们已经主从读写分离。 像刀一样，越磨越亮我们在新的阶段探研了新的技术 使用ambari管理各种集群 探索数据流任务管理平台，对比了kettle，apache nifi和spring cloud data flow 研究能够快速检索数据且能够横向扩展的技术，如elasticsearch,kafka等 过往我们都是存储及计算一体，像我们对mysql,hive的应用就是典型的先把数据迁移到哪里再进行计算，所以我们也探研了计算分离的技术方案，最终采用presto 对于快速发展的创业公司来说好用才是王道，大家舒服才是王道&ensp; &ensp;我们的数据量始终没有达到TB级别，可预见如果业务没有新的发展方向，就当前的产品的数据增长速度来看，可以把我们要做的数据平台定义为“小数据量的大数据平台”，机器资源也只是近10台（我知道市场正常搞大数据的机器集群一般都有几十上百台，再次手动捂脸）。最终确定我们架构的方向: 使用otter实时同步我们所有业务数据到我们自己的数据库，减少离线同步的管理和资源成本 使用canal 监听自己数据库的所有表变化，程序同步序列化数据到kafka中，由业务方使用kafka streams自行订阅，实现业务逻辑 使用azkaban + datax + presto ，实现分布式计算，分离存储与计算环节，完成etl流程 最终离线数据落地elasticsearch 业务方通过数据平台进行数据需求的自满足及溯源 我们当前的架构如下(蓝色箭头为源头输出，红色箭头指向为输入，黄色箭头为溯源流向，黄色闪电为依存关系): 而最终稳定版的架构如下: 智能化的业务&ensp; &ensp;近来公司重点发展智能化业务，需要我们支持机器学习相关的算法模型实现及调优，目前还是处于使用python脚本的实现方式及azkaban进行任务训练及提供脚本给程序调用。一方面还不是分布式计算，当然我们也可以用spark解决这个问题，第二方面，支持不了实时训练的场景，虽然我们当前也还没有这种需求。目前这块还是架构规划当中，可能会比较倾向于使用tensorflow,后续有进展再更新。 感想&ensp; &ensp; 我始终没觉得自己是一个大数据开发人员，在我看来，web开发与数据开发同样是后端研发，只不过业务不同，使用的技术不同，然而很多研发的基础是一样。语言基础，高并发，大数据量，大吞吐量，分布式，高可用，模式设计，基础算法，调优，使用的工具掌握及理解像redis,zk,es,kafka,mysql这些，运维基础，前端基础，像这些我觉得都是应该掌握的。&ensp; &ensp; 面试过很多大数据的开发，从初级到高级。很多人都只关心一个问题，你们的数据量有多大，你们的集群有多大。我承认这些这两个“指标”确实能说明一些东西，但是对于我面试过的人来说，我的结论是，数据量的大小对于只是一个使用工具的人来说，其实并不重要，因为你也只是用，你从来不深究，为什么要这样设计，为什么要这样实现，有没有办法做得更好，你不了解原理，你不熟悉工具技术，反而看不起数据量小的业务场景，这会让人觉得不成熟。&ensp; &ensp; 当然我也是认为如果有机会接触真正的大量数据的场景，还是得尽量接触，在保证自己有不断提升的觉悟为前提去接触。面试过很多游戏通信行业的真正大数据量场景下的小伙伴，真的不得不提一句，你公司的业务场景只是一方面，不“用”起来，只是负责某个环节的加工还不深究，这样的开发真的不算是大数据研发啊。&ensp; &ensp; peace &amp; love]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>数据平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天然苏打水市场的了解与分析]]></title>
    <url>%2F2018%2F08%2F14%2F%E5%A4%A9%E7%84%B6%E8%8B%8F%E6%89%93%E6%B0%B4%E5%B8%82%E5%9C%BA%E7%9A%84%E4%BA%86%E8%A7%A3%E4%B8%8E%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[背景近来有个朋友找到我说拿到某品牌的天然苏打水广东省省级代理，问我得怎样开展工作及市场上的情况是怎么样的。12产品定价15元一支品牌方给出了一年一千万件，即八千万支的销量目标。 table td {text-align:center} table thead td {background:#73B1E0;color:#FFF;} table th {border:1}由于个人是在TO B电商平台工作（苦逼码农一个），就着自己的资源和能力稍微进行一下分析。目标是找出工作推进的方案和尽量找出市场数据以及判断这个销量kpi是否合理。 梳理步骤 了解市场竞品数据 了解一般水饮新品推广方法 了解行业高端专业人士对这个事件的看法 了解市场竞品数据找遍关系圈并没有任何可以了解到天然苏打水相关的线下渠道，于是只能针对线上电商平台先下手。挑选了天然苏打水销量量较大的平台的销量较好的四个产品进行对比。结果如下: 产品 产品单价 规格 活动 京东销量 天猫销量 品牌 产品卖点 产地 公司 公司旗下产品 公司官网 新闻备注 品牌故事 舒达源克东天然苏打水 9.5 550ml 买2送1（送6支400ml） 1.9w评价 2758 舒达源 世界三大冷矿泉之一 克东 黑龙江舒达饮品有限公司 单一产品不同规格 跳转 新闻1 中国国家田径队官方用水 活力恩克东天然苏打水 3.66 500ml 无 3.3w评价 4542(单价4.5) 活力恩 火山岩層精淬礦泉 克东 海昌國際股份有限公司（彰化縣秀水鄉） 主打5度C系列，销量情况一般 跳转 无 无 火山鸣泉克东天然苏打水 7.3 470ml 无 1.7w评价 3578 火山鸣泉 火山岩層精淬礦泉 克东 火山鸣泉生态科技有限公司 单一产品不同规格不同包装 跳转 新闻1 新闻2 中国田径队官方饮用水 水易方克东天然苏打水 7.1 500ml 买5送1 1.5w评价 537 水易方 常见天然苏打水特性 克东 大连水易方科技发展有限公司 单一产品不同规格不同包装 跳转 新闻 无 就这样看可以得到一些信息12345* 京东是现在最大的销售渠道，天猫第二（其它平台几乎没有销量所以没对比）* 天然苏打水线上市场并不乐观,如果按这数据来看，一年100w件的销量目标都很有挑战性* 国内最有知名度的天然苏打水产地应该是克东，如果非克东产地的苏打水估计比较难引起消费者共鸣* 产品形象及品牌故事在市场竞争中影响并不大，消费者可能更看重实惠程度* 基本没看到哪个天然苏打水产品有打广告，更多应该是参加电商平台的促销活动 基本上线上销售相关信息了解就到这了，但是感觉还不够，进而找了各大数据平台搜索相关报告，收集到资料如下: 网上评价中国十大苏打水企业http://www.china-10.com/china/1002sds_index.html 中国会员经济数据报告http://tech.qq.com/a/20170719/007724.htm#p=1 尼尔森数据线上线下结合销售数据资讯http://www.nielsen.com/cn/zh/press-room/2015/Nielsen-global-survey-on-e-commerce-and-new-retailing.html 百度指数http://index.baidu.com/?tpl=trend&amp;word=%CB%D5%B4%F2%CB%AE 腾讯BIhttp://tbi.tencent.com/index?word=苏打水&amp;date=1&amp;type=0 36kr关于高端水的观点http://36kr.com/p/5073894.html 3mbang文库http://www.3mbang.com/p-171349.html 百度文库https://wenku.baidu.com/view/adb970a03968011ca200911f.htmlhttps://wenku.baidu.com/view/64a25f0869eae009581becfb.htmlhttp://www.chinairn.com/news/20160705/150700863.shtml ps:苏打水市场研究报告 很多咨询网站都有做，但是哪里都要钱的，一份就要好几k大洋，要不起，还是靠自己吧。 基于网上的资料可以看出来苏打水在中国的市场热度有上升的趋势，但是不大，天然苏打水就更小了。 一般水饮新品推广方法我找了我们公司运营部门副总监W君请教了一下，他原本是从宝洁出来的，至今工作多年，从开始的市场人员转为运营人员。从与他的交流上我总结了一下代理商一般水饮新品的推广方法如下: KA(大型卖场)如沃尔玛、华润万家、永旺（吉之岛）等。 KA是最容易进行新品试验的地方。每个KA都有自己招商的标准，对进场的产品会有不同的要求，KA每个点（如广州天河分店等）也各自有自己的要求。一般想谈合作可以直接到联系总部或者分点对应负责人直接谈就行了。 这里假设能够满足要求并进场。合作的对象有两种，一种是直接找总部谈合作，这种方式会要求产品本身有足够强的优势，总公司才会考虑帮你铺点，而且一般找总部谈的整个流程比较长，但是一旦确定产品OK，能够大面积铺开，另一种是找某个KA的分点负责人谈，这种的门槛会比较低，但是产品流通只限于该门店，如果想再扩大市场只能自己重新去谈其它KA分点。 合作的方式有两种，一种是直接给钱进场，KA不管你销量，反正它有钱收，另一种是根据销量来进行利润分成，这种一般只会针对成熟的产品或者非常有潜力的产品。 走KA的公司的运营比较简单，只需要有人负责促销活动的规划，聘请对应的促销人员派到各大卖场进行促销活动就可以了。 然而KA渠道基本就是走量，利润空间比较小。 城市有能力的中间商如广州宝祺来，专门做日化渠道，能够快速铺货到各大KA卖场，合作方式得自己谈，基本是直接给钱合作的。 这个渠道会比直接找KA利润更低，因为有中间商赚差价嘛。但是铺货速度跟能力绝对是杠杠的。 特殊渠道带有特殊性质的饮品，如能量饮料、苏打水，都有一些特殊的渠道可以卖，这个渠道比较看重经营方即卖饮品的公司渠道能力。比如红牛可以铺货进网吧、各大运动场所等。这个渠道我也没有资源可以继续了解，只是知道，特殊渠道是毛利可以爆发的渠道。 中小超市士多基本上品牌代理商不会自己直接去铺小店，成本高，效益低。 专业人士意见我找了我们公司采购部副总监请教，他是一路从恒大、沃尔玛等大公司由基层做到高层的。沟通下来我总结有以下几点： 新品不能急着铺货，只要你产品OK，销售渠道是比较容易疏通的 确定好主打市场，目标人群 做好市场调研，可以采用问券调查等方式。如果真的不清晰怎么做，可以拜访其它省级代理，咨询下人家的情况与做法 商品本身的主打特性要宣传到位，假如商品本身有什么特别性质的，如真的可以治疗什么疾病或者产源稀缺，那定价高一样可以卖得火爆 特殊渠道在定位高端产品的渠道中比较重要，往往是有利可图的渠道 15元的高端水在市场很少见，至少这位哥是没见过，当时在恒大做恒大冰泉也是定价高昂，现在一样卖到跟普通差不多，正常渠道销量也一般。所以这个定价的水产品市场不好做 总结针对销量目标来说，天然苏打水新品一年一千万件即八千箱的省级销售目标，从运营和专业人士的角度来看，都是很难达到，结合线上数据来看，也更加能说明这点。合理的有挑战性的目标大概是一百万件左右。假如有特殊渠道另说，因为特殊渠道的量就看主营公司的能力了，不好预估。推广新品的方法上面已经提到，但是推广前需要确定好目标人群、商品的亮点、品牌故事与背景和渠道。如果不清楚怎么做，可以去其它省级代理了解咨询，如果有的话。 吐槽基本上KA或者平台级的公司不会管你产品好不好卖，都想你投钱进场，反正不好卖也不关他们的事。包括我们公司那采购副总监，一边说着感觉市场不好做，一边想让我朋友来找我们公司合作，笑哭，哈哈]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>生意分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud stream 基于kafka的使用简析]]></title>
    <url>%2F2018%2F08%2F11%2Fspring-cloud-stream-%E5%9F%BA%E4%BA%8Ekafka%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[流式数据故名思义，即数据像开了小河里的流水般不停流动，除非水源出现问题，否则没有结束时间。流式数据在行业内已经有非常多针对不同应用量级的成熟方案，这里就不加以详述。本次主要介绍spring cloud stream 基于kafka对流式数据的基本应用。而使用spring cloud stream之前，可以先理解一下spring cloud对数据流程的几个概念，分别是source（生产数据者），processor(数据加工者), sink(最终结果处理者)。 准备工作项目基本框架:当然是基于maven构建的spring-boot最省心省力啦添加依赖:1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka-streams&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 用于代码中的一些便捷注解 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 公共配置:1234567891011121314151617181920spring: cloud: kafka: streams: binder: brokers: localhost:9092 zk-nodes: localhost:2181 #2.0以上就不需要该配置 configuration: default: key: serde: org.apache.kafka.common.serialization.Serdes$StringSerde value: serde: org.apache.kafka.common.serialization.Serdes$StringSerde cache: max: bytes: buffering: 0 #所有线程可以用于缓存的最大字节数,达到多少数据量之后聚合一次流中的数据，设置为0则实时聚合 commit: interval: ms: 1000 #版本数据确认消费时间ack 我们来模拟用户页面访问记录流 声明binding相关信息添加接口AnalyticsBinding1234567891011121314151617181920public interface AnalyticsBinding &#123; String PAGE_VIEWS_OUT = &quot;pvout&quot;; String PAGE_VIEWS_IN = &quot;pvin&quot;; String PAGE_COUNT_MV = &quot;pcmv&quot;; String PAGE_COUNT_OUT = &quot;pcout&quot;; String PAGE_COUNT_IN = &quot;pcin&quot;; @Input(PAGE_VIEWS_IN) KStream&lt;String,PageViewEvent&gt; pageViewsIn(); @Output(PAGE_COUNT_OUT) KStream&lt;String,Long&gt; pageCountOut(); @Input(PAGE_COUNT_IN) KTable&lt;String,Long&gt; pageCountIn(); @Output(PAGE_VIEWS_OUT) MessageChannel pageViewsOut();&#125; 添加配置在application.yml中声明以下配置12345678910111213141516171819202122232425262728293031323334353637spring: cloud: stream: bindings: pvout: destination: pvst group: pvst producer: header-mode: raw pvin: destination: pvst group: pvst consumer: header-mode: raw pcout: destination: pcst group: pcst producer: use-native-encoding: true pcin: destination: pcst group: pcst content-type: application/json consumer: use-native-encoding: true header-mode: raw kafka: streams: bindings: pcout: producer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde pcin: consumer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde destination对应kafka中的主题。serde就是serialization和deserialization，针对流中的key和value都需要指定，默认使用default中配置的内容，也可以针对主题单独设置。 source创建每秒随机产生用户访问页面及停留时间的数据 先创建事件类1234567@Data@AllArgsConstructor@NoArgsConstructorpublic class PageViewEvent &#123; private String userId,page; private long duration;&#125; 生成数据的实现123456789101112131415161718192021222324252627282930313233@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class PageViewEventSource implements ApplicationRunner &#123; private final MessageChannel pageViewsOut; public PageViewEventSource(AnalyticsBinding binding) &#123; this.pageViewsOut = binding.pageViewsOut(); &#125; @Override public void run(ApplicationArguments applicationArguments) throws Exception &#123; List&lt;String&gt; names = Arrays.asList(&quot;jlong&quot;,&quot;dyser&quot;,&quot;shacko&quot;,&quot;abilan&quot;,&quot;ooasdf&quot;,&quot;grussell&quot;); List&lt;String&gt; pages = Arrays.asList(&quot;blog&quot;,&quot;sitemap&quot;,&quot;initializr&quot;,&quot;news&quot;,&quot;colophon&quot;,&quot;about&quot;); Runnable runnable = () -&gt; &#123; String rPage = pages.get(new Random().nextInt(pages.size())); String rName = names.get(new Random().nextInt(names.size())); PageViewEvent pageViewEvent = new PageViewEvent(rName,rPage,Math.random() &gt; .5?10:1000); Message&lt;PageViewEvent&gt; message = MessageBuilder.withPayload(pageViewEvent) .setHeader(KafkaHeaders.MESSAGE_KEY,pageViewEvent.getUserId().getBytes()) .build(); try &#123; this.pageViewsOut.send(message); log.info(&quot;sent&quot; + message.toString()); &#125; catch (Exception e)&#123; log.error(e.getMessage()); &#125; &#125;; Executors.newScheduledThreadPool(1).scheduleAtFixedRate(runnable,1,1, TimeUnit.SECONDS); &#125;&#125; processor获取到事件数据之后基于聚合处理，创建新的数据流123456789101112131415@Slf4j@Componentpublic class PageViewEventProcessor &#123; @StreamListener @SendTo(AnalyticsBinding.PAGE_COUNT_OUT) public KStream&lt;String,Long&gt; process( @Input(AnalyticsBinding.PAGE_VIEWS_IN) KStream&lt;String,PageViewEvent&gt; events)&#123; return events .map((key,value) -&gt; new KeyValue&lt;&gt;(value.getUserId() + &quot;-&quot; + value.getPage(),&quot;0&quot;)) .groupByKey() //.windowedBy(TimeWindows.of(1000*60)) .count(Materialized.as(AnalyticsBinding.PAGE_COUNT_MV)) .toStream(); &#125;&#125; windowedBy可以根据时间窗口进行聚合，用法请详见文档。 sink获取聚合后的结果进行处理123456789@Slf4j@Componentpublic class PageCountSink &#123; @StreamListener public void process(@Input(AnalyticsBinding.PAGE_COUNT_IN)KTable&lt;String,Long&gt; counts)&#123; counts.toStream().foreach((key,value) -&gt; log.info(&quot;PCIN -----:&quot; + key + &quot;=&quot; + value)); &#125;&#125; 至此一个从数据生产到结果消费的简单数据流处理就完成了 解析这个例子是基于spring cloud 完整的流数据处理，有source,processor,sink的概念是spring cloud data flow的设计理念，这里不展开阐述。processor环节非必需的，可以只有source和sink的实现。假如不需要进行流的处理，只需要消息内容，可以在@StreamListener的方法声明中不使用@Input声明，而是直接通过@StreamListener(主题名称)来进行监听，方法接收消息参数使用Message msg，如下: 123456789@StreamListener(Processor.INPUT) public void receive1(Message&lt;String&gt; msg)&#123; System.out.println(msg.getPayload()); //消息体 Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class); if (acknowledgment != null) &#123; System.out.println(&quot;Acknowledgment provided&quot;); acknowledgment.acknowledge();//手动ack &#125; &#125; 多流聚合在实际应用中不只存在单流数据的处理，也经常会遇到多流聚合处理。我们来添加多一种事件的实现，这里模拟销售数据。相关的实现如下 SalesEvent1234567@Data@AllArgsConstructor@NoArgsConstructorpublic class SalesEvent &#123; private String userId,goods; private int amount;&#125; SalesEventSource12345678910111213141516171819202122232425262728293031@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class SalesEventSource implements ApplicationRunner &#123; private final MessageChannel salesOut; public SalesEventSource(AnalyticsBinding binding) &#123; this.salesOut = binding.salesOut(); &#125; @Override public void run(ApplicationArguments applicationArguments) throws Exception &#123; List&lt;String&gt; names = Arrays.asList(&quot;jlong&quot;,&quot;dyser&quot;,&quot;shacko&quot;,&quot;abilan&quot;,&quot;ooasdf&quot;,&quot;grussell&quot;); List&lt;String&gt; goods = Arrays.asList(&quot;apple&quot;,&quot;oringe&quot;,&quot;banana&quot;,&quot;lemon&quot;,&quot;shit&quot;,&quot;book&quot;); Runnable runnable = () -&gt; &#123; String rGoods = goods.get(new Random().nextInt(goods.size())); String rName = names.get(new Random().nextInt(names.size())); SalesEvent salesEvent = new SalesEvent(rName,rGoods,Math.random() &gt; .5?5:10); Message&lt;SalesEvent&gt; message = MessageBuilder.withPayload(salesEvent) .setHeader(KafkaHeaders.MESSAGE_KEY,salesEvent.getUserId().getBytes()) .build(); try &#123; this.salesOut.send(message); log.info(&quot;sent&quot; + message.toString()); &#125; catch (Exception e)&#123; log.error(e.getMessage()); &#125; &#125;; Executors.newScheduledThreadPool(1).scheduleAtFixedRate(runnable,1,1, TimeUnit.SECONDS); &#125;&#125; SalesEventProcessor123456789101112131415@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class SalesEventProcessor &#123; @StreamListener @SendTo(AnalyticsBinding.SALES_COUNT_OUT) public KStream&lt;String,Long&gt; process(@Input(AnalyticsBinding.SALES_IN)KStream&lt;String,SalesEvent&gt; events)&#123; return events .map((key,value) -&gt; new KeyValue&lt;&gt;(value.getUserId() + &quot;-&quot; + value.getGoods(),&quot;0&quot;)) .groupByKey() //.windowedBy(TimeWindows.of(1000*60)) .count(Materialized.as(AnalyticsBinding.SALES_COUNT_MV)) .toStream(); &#125;&#125; SaleCountSink123456789101112131415161718@Slf4j@Component@EnableBinding(AnalyticsBinding.class)public class SaleCountSink &#123; @StreamListener public void process(@Input(AnalyticsBinding.SALES_COUNT_IN)KTable&lt;String,Long&gt; salesCounts, @Input(AnalyticsBinding.PAGE_COUNT_IN)KTable&lt;String,Long&gt; pageCounts)&#123; salesCounts.toStream().map((k,v) -&gt; new KeyValue&lt;&gt;(k.split(&quot;-&quot;)[0],k.split(&quot;-&quot;)[1] + &quot;-&quot; + v)) .join(pageCounts.toStream().map((k,v) -&gt; &#123; System.out.println(&quot;-------&quot; + k +&quot; : &quot; + v ); return new KeyValue&lt;&gt;(k.split(&quot;-&quot;)[0],k.split(&quot;-&quot;)[1] + &quot;-&quot; + v);&#125;), (v1, v2) -&gt; v1 + &quot;:&quot; + v2, JoinWindows.of(10000) ) .foreach((k,v) -&gt; System.out.println(k+ &quot;---&quot; + v)); &#125;&#125; 注意：同个应用中对同个流的监听实例只能有一个，SaleCountSink使用了@Input(AnalyticsBinding.PAGE_COUNT_IN)KTable&lt;String,Long&gt;与PageCountSink有冲突，要嘛把PageCountSink中的监听去掉，要嘛重命名其中一个相关的监听配置 AnalyticsBinding 完整代码12345678910111213141516171819202122232425262728293031323334353637public interface AnalyticsBinding &#123; String PAGE_VIEWS_OUT = &quot;pvout&quot;; String PAGE_VIEWS_IN = &quot;pvin&quot;; String PAGE_COUNT_MV = &quot;pcmv&quot;; String PAGE_COUNT_OUT = &quot;pcout&quot;; String PAGE_COUNT_IN = &quot;pcin&quot;; String SALES_OUT = &quot;salesout&quot;; String SALES_IN = &quot;salesin&quot;; String SALES_COUNT_MV = &quot;scmv&quot;; String SALES_COUNT_OUT = &quot;scout&quot;; String SALES_COUNT_IN = &quot;scin&quot;; @Input(PAGE_VIEWS_IN) KStream&lt;String,PageViewEvent&gt; pageViewsIn(); @Output(PAGE_COUNT_OUT) KStream&lt;String,Long&gt; pageCountOut(); @Input(PAGE_COUNT_IN) KTable&lt;String,Long&gt; pageCountIn(); @Output(PAGE_VIEWS_OUT) MessageChannel pageViewsOut(); @Output(SALES_OUT) MessageChannel salesOut(); @Input(SALES_IN) KStream&lt;String,SalesEvent&gt; salesIn(); @Output(SALES_COUNT_OUT) KStream&lt;String,Long&gt; salesCountOut(); @Input(SALES_COUNT_IN) KTable&lt;String,Long&gt; salesCountIn();&#125; application.yml完整配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990server: port: 8388spring: application: name: input-demo cloud: instance-count: 1 instance-index: 0 stream: bindings: pvout: destination: pvst group: pvst producer: header-mode: raw pvin: destination: pvst group: pvst consumer: header-mode: raw pcout: destination: pcst group: pcst producer: use-native-encoding: true pcin: destination: pcst group: pcst content-type: application/json consumer: use-native-encoding: true header-mode: raw salesout: destination: sost group: sost producer: header-mode: raw salesin: destination: sost group: sost consumer: header-mode: raw scout: destination: scst group: scst producer: use-native-encoding: true scin: destination: scst group: scst content-type: application/json consumer: use-native-encoding: true header-mode: raw kafka: streams: binder: configuration: default: key: serde: org.apache.kafka.common.serialization.Serdes$StringSerde value: serde: org.apache.kafka.common.serialization.Serdes$StringSerde cache: max: bytes: buffering: 0 commit: interval: ms: 1000 brokers: localhost:9092 zk-nodes: localhost:2181 bindings: pcout: producer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde pcin: consumer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde scout: producer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde scin: consumer: key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde value-serde: org.apache.kafka.common.serialization.Serdes$LongSerde 多流合并的理念是把流两两之间的key处理成一样进行join处理，join的实现方法大家可以自行阅读文档。如果是三个流以上，需要先将两个流合并之后生成一个流再与第三流合并处理。 相关文档 spring cloud data flow spring cloud stream spring cloud stream kafka streams应用官方视频讲解 KTable与KStream的关系 kafka官方文档 kafka streams window的概念翻译版 kafka权威指南]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>实时流</tag>
        <tag>spring</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存与JVM]]></title>
    <url>%2F2018%2F07%2F23%2F%E5%86%85%E5%AD%98%E4%B8%8EJVM%2F</url>
    <content type="text"><![CDATA[自己一边看书一边实践一边整理下来的知识思维导向图，个人感觉能够掌握自动内存管理机制、高效并发并加以应用，理解虚拟机执行子系统、程序编译与代码优化，才可以说熟悉JVM。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java基础</tag>
        <tag>JVM</tag>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zk分布式任务队列交互设计]]></title>
    <url>%2F2018%2F07%2F22%2Fzk%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97%E4%BA%A4%E4%BA%92%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[项目地址：https://github.com/super-sean/task-pipeline 背景近来公司业务上越来越多的跨进程比较耗时计算的场景出现，想用异步通信来解决长时间资源占用及等待问题，而基于多方的探讨，不考虑采用mina和netty这种异步通信的框架，最后决定使用zookeeper来实现 目的异步进行耗时较长的复杂计算请求，可随时获取请求执行进度 实现思路 将这种请求的发起者当作是请求任务的生产者，每个请求其实就是一个计算任务。 后端接收请求的服务就是消费者，获得请求之后进行计算，并更新计算进度。 当任务完成时，请求发起者可以通过监听任务状态回调实现自己的逻辑。 过程中，请求发起者也可以主动获取计算讲求的进度。 实现设计基于实现思路，设计zk的path结构如下 /master为程序高可用实现预留路径 /apps为业务连接节点，底下结构为/app/node，比如你有个业务叫a,有两个业务节点b1和b2，那就有/a/b1和/a/b2 路径。由业务节点启动时注册 /workers底下结构逻辑与/apps一致，只不过节点为服务端的节点，由服务端节点启动时注册 /tasks由业务提交注册的计算任务,以业务区分目录，以app-node-timestamp格式来命名taskid,每个节点拥有params,status和result三个节点 params 为请求参数，以文本格式存储，例如可以使用json格式传输 status 为task状态，默认有submit,running,done,noworker（无计算服务）,missapp（app节点断线）,consumed（已消费），resubmit（重分配）几种状态，worker可以添加自定义中间过程状态，任务提交时默认为submit状态。 result 为初始不存在，当status变更为done时添加，内容为文本格式，例如可以使用json，包括type和value,先只支持两种，第一种为直接返回为{“type”:”content”,”value”:”something”},考虑zk单个节点的容量问题，可能返回较大数据量，使用redis作为结果缓存层，返回{“type”:”redis_key”,”value”:”one redis key”} 当然不用redis也行，当数据量更大的时候可使用其它工具，这里先选用redis history目录下为完成的任务，定时持久化清理。 /assign由系统根据业务app分配作业给worker，以node-taskid来标识作业history目录下为执行完的作业，定时持久化清理 模块设计 调度系统 实现基于zk的路径交互，负责与业务和服务两端交互 业务端接口包封装 对于业务端来说，只需要提交服务端接口标识，接口参数之后返回taskId,根据需要通过taskId进行结果回调监听，支持查询task状态，需要屏蔽底层操作，透明化复杂操作。 服务端接口包封装 对于服务端来说，只需要继承某个类，声明服务标识，实现监听task队列的方法，处理被推送过来的任务，并根据需要更新自定义task状态，处理完成后在方法选择返回的内容类型即可 流程设计正常交互流程(由于用的uml画图工具问题，画得不是很规范，见谅…)正常交互流程worker断线重新分配任务流程 核心模块类图基本操作都抽象成名为operation的类，基于不同角色做扩展，目前情况如下baseOperation为zk的基本操作，operation为倾向原子性业务操作，分角色扩展的operation如serverOperation为封装角色实现本身的组合操作监听器主要有以下监听器实现每个角色都是基于以上两个核心模块加以逻辑处理来实现自己的功能 其它相关设计Task分发策略worker每当被分发task，便权重添加1，处理完则减1分发Task时选择权重最小的节点若权重都一样，则选择第一个节点 server主从实现使用curator包的LeaderLatch zk path acl权限管理使用三个角色，tp_server,tp_worker,tp_app目前没有做细粒度控制，只是tp_server创建的给另外两个角色授权，tp_worker创建的给tp_server授权，tp_app创建的给tp_server授权]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>分布式</tag>
        <tag>队列</tag>
      </tags>
  </entry>
</search>
